{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb502b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f110e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeee0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    'ğŸ˜€': 'feliz ',\n",
    "    'ğŸ˜‚': 'risos ',\n",
    "    'ğŸ˜”': 'triste ',\n",
    "    'ğŸ‘': 'palmas ',\n",
    "    'ğŸ¥°': 'amÃ¡vel ',\n",
    "    'ğŸ’™': 'coraÃ§Ã£o azul ',\n",
    "    'ğŸ™ğŸ¼': 'orando ',\n",
    "    'âœ¨': 'brilhando ',\n",
    "    'ğŸ¤®': 'nojo ',\n",
    "    'ğŸš€': 'foguete ',\n",
    "    'ğŸ‘¿': 'diabo ',\n",
    "    'ğŸ¤¢': 'nojo ',\n",
    "    'ğŸ”¥': 'fogo ',\n",
    "    'ğŸ˜¡': 'fÃºria ',\n",
    "    'ğŸ˜ ': 'raiva ',\n",
    "    'ğŸ¤£': 'rindo ',\n",
    "    'ğŸ˜ƒ': 'feliz ',\n",
    "    'ğŸ˜': 'curtindo ',\n",
    "    'ğŸ˜Š': 'feliz ',\n",
    "    'ğŸ¤©': 'maravilhado ',\n",
    "    'ğŸ˜‹': 'delicioso ',\n",
    "    'ğŸ˜†': 'risada ',\n",
    "    'ğŸ˜Œ': 'calmo ',\n",
    "    'ğŸ¤”': 'pensativo ',\n",
    "    'ğŸ˜·': 'mÃ¡scara ',\n",
    "    'ğŸ¤£': 'muitoRiso ',\n",
    "    'ğŸ¥º': 'carinhoso ',\n",
    "    'ğŸ‘': 'positivo ',\n",
    "    'ğŸ¤¯': 'menteExplodida ',\n",
    "    'ğŸ˜…': 'alÃ­vio ',\n",
    "    'ğŸ¥°': 'carinhaComCoraÃ§Ã£o ',\n",
    "    'ğŸ˜“': 'suor ',\n",
    "    'ğŸ˜‘': 'tÃ©dio',\n",
    "    'ğŸ¤«': 'silÃªncio',\n",
    "    'ğŸ¤': 'apertoDeMÃ£os',\n",
    "    'ğŸ˜Š': 'sorriso',\n",
    "    'ğŸ˜': 'apaixonado',\n",
    "    'ğŸ˜­': 'choro ',\n",
    "    'ğŸ¤—': 'abraÃ§o ',\n",
    "    'ğŸ‰': 'festa ',\n",
    "    'ğŸ˜': 'descolado ',\n",
    "    'ğŸ˜±': 'surpresa ',\n",
    "    'ğŸ˜´': 'sono ',\n",
    "    'ğŸ™Œ': 'celebraÃ§Ã£o ',\n",
    "    'ğŸ¤”': 'pensativo ',\n",
    "    'ğŸ˜˜': 'beijo ',\n",
    "    'ğŸ¥³': 'festeiro ',\n",
    "    'ğŸ™„': 'revirarOsOlhos ',\n",
    "    'ğŸ˜Œ': 'alÃ­vio ',\n",
    "    'ğŸ¤«': 'segredo ',\n",
    "    'ğŸ˜‡': 'inocente ',\n",
    "    'ğŸ˜‚': 'muitoEngraÃ§ado ',\n",
    "    'ğŸ¤”': 'pensando ',\n",
    "    'ğŸ˜´': 'sono ',\n",
    "    'ğŸ¤ª': 'loucura ',\n",
    "    'ğŸ˜¢': 'decepcionadoAliviado ',\n",
    "    'ğŸ˜¬': 'nervoso ',\n",
    "    'ğŸ˜Œ': 'alÃ­vio',\n",
    "    'ğŸ˜”': 'triste ',\n",
    "    'ğŸ˜': 'desapontado ',\n",
    "    'ğŸ˜¢': 'choro ',\n",
    "    'ğŸ˜­': 'chorando ',\n",
    "    'ğŸ˜¡': 'raiva ',\n",
    "    'ğŸ¤¯': 'mente explodida ',\n",
    "    'ğŸ˜³': 'surpreso ',\n",
    "    'ğŸ˜±': 'gritando ',\n",
    "    'ğŸ˜¨': 'assustado ',\n",
    "    'ğŸ˜´': 'sono ',\n",
    "    'ğŸ¥±': 'bocejando ',\n",
    "    'ğŸ¤¢': 'enjoado ',\n",
    "    'ğŸ¤®': 'vomitando ',\n",
    "    'ğŸ¤§': 'espirro ',\n",
    "    'ğŸ¤’': 'doente ',\n",
    "    'ğŸ¤•': 'machucado ',\n",
    "    'ğŸ¤‘': 'dinheiro ',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56695fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_word(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        palavras = texto.split()\n",
    "        texto_processado = []\n",
    "        for palavra in palavras:\n",
    "            if palavra in emoji_dict:\n",
    "                texto_processado.append(emoji_dict[palavra])\n",
    "            else:\n",
    "                texto_processado.append(palavra)\n",
    "        texto_processado = ' '.join(texto_processado)\n",
    "        textos_processados.append(texto_processado)\n",
    "    \n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88c240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarTexto(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        texto = texto.lower()\n",
    "        tokens = word_tokenize(texto)\n",
    "        stop_words = [\n",
    "            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',\n",
    "            'n', 'a', 'Ã ', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',\n",
    "            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',\n",
    "            'aquelas', 'aqui', 'aquilo', 'cÃ¡', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',\n",
    "            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',\n",
    "            'nÃ³s', 'vocÃªs', 'voces', 'enquanto', 'era', 'estÃ¡', 'estamos', 'estÃ£o', 'estar', 'estarÃ¡', 'estive', 'estivemos',\n",
    "            'estiver', 'estivera', 'estiveram', 'estivÃ©ramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',\n",
    "            'estivÃ©ssemos', 'estiveste', 'estivestes', 'estou', 'farÃ¡', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',\n",
    "            'for', 'fora', 'foram', 'fÃ´ramos', 'forem', 'formos', 'fosse', 'fossem', 'fÃ´ssemos', 'foste', 'fostes', 'fui',\n",
    "            'fÃ´ssemos', 'hÃ¡', 'houve', 'hoje', 'isso', 'isto', 'jÃ¡', 'lÃ¡', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',\n",
    "            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',\n",
    "            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',\n",
    "            'ninguÃ©m', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',\n",
    "            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'porÃ©m', 'porque', 'porquÃª', 'prÃ³pria', 'prÃ³prio',\n",
    "            'prÃ³prias', 'prÃ³prios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'quÃª', 'quem', 'quer', 'quereis',\n",
    "            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quisÃ©ramos', 'quiserem',\n",
    "            'quisermos', 'quisÃ©sseis', 'quisÃ©ssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',\n",
    "            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quizÃ©ssemos', 'sÃ£o', 'se', 'seja', 'sejam',\n",
    "            'sejamos', 'sem', 'sendo', 'ser', 'serÃ¡', 'serÃ£o', 'serÃ¡', 'seriam', 'serÃ­amos', 'serias', 'serÃ­eis', 'sete',\n",
    "            'seu', 'seus', 'sob', 'sobre', 'sois', 'sÃ³', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'tambÃ©m', 'te',\n",
    "            'tem', 'tÃªm', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terÃ¡', 'terÃ£o',\n",
    "            'terÃ¡', 'teriam', 'terÃ­amos', 'terias', 'terÃ­eis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',\n",
    "            'tiveram', 'tivÃ©ramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivÃ©ssemos', 'tiveste', 'tivestes',\n",
    "            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns'\n",
    "        ]\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in stop_words and not token.startswith('@') and token.isalpha()\n",
    "        ]\n",
    "        textos_processados.append(tokens)\n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7f9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizar_frases(frases, dictionary):\n",
    "    frases = [' '.join(tokens) for tokens in frases]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.set_params(vocabulary=dictionary)\n",
    "    frases_vetorizadas = vectorizer.fit_transform(frases)\n",
    "    return frases_vetorizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b435bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_naive_bayes.pkl', 'rb') as file:\n",
    "    modelo_carregado = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dicionÃ¡rio usando pickle\n",
    "with open('dictionary.pkl', 'rb') as file:\n",
    "    dictionary2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c57da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e7b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@app.route('/classificar', methods=['POST'])\n",
    "def classificar():\n",
    "    dados = request.json\n",
    "    # Aplique a funÃ§Ã£o emoji_to_word() aos dados do web scraping\n",
    "    textos_entrada = dados[\"dados\"]\n",
    "    textos_processados = emoji_to_word(textos_entrada)\n",
    "    textos_processados = processarTexto(textos_processados)\n",
    "    frases_vetorizadas = vetorizar_frases(textos_processados, dictionary2)\n",
    "    frases_vetorizadas = frases_vetorizadas.toarray()  # Converter para matriz densa\n",
    "    predicoes = modelo_carregado.predict(frases_vetorizadas)\n",
    "\n",
    "    # Mapear valores numÃ©ricos para palavras correspondentes\n",
    "    mapeamento_classes = {0: \"negativo\", 1: \"neutro\", 2: \"positivo\"}\n",
    "    predicoes_palavras = [mapeamento_classes[predicao] for predicao in predicoes]\n",
    "\n",
    "    return json.dumps(predicoes_palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3000c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@app.route('/proporcoes', methods=['POST'])\n",
    "def proporcoes():\n",
    "    predicoes_palavras = request.json\n",
    "    predicoes_palavras = predicoes_palavras[\"dados\"]\n",
    "\n",
    "    # Mapear os sentimentos para os valores numÃ©ricos correspondentes\n",
    "    mapeamento_sentimentos = {\"negativo\": 0, \"neutro\": 1, \"positivo\": 2}\n",
    "    predicoes_numeros = [mapeamento_sentimentos[sentimento] for sentimento in predicoes_palavras]\n",
    "\n",
    "    # Contar a ocorrÃªncia de cada sentimento\n",
    "    proporcoes = Counter(predicoes_numeros)\n",
    "\n",
    "    # Calcular as proporÃ§Ãµes\n",
    "    total = len(predicoes_numeros)\n",
    "    proporcoes = {sentimento: count/total for sentimento, count in proporcoes.items()}\n",
    "\n",
    "    return json.dumps(proporcoes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db97b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@app.route('/nuvem-palavras', methods=['POST'])\n",
    "def nuvem_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "    \n",
    "    # Unir todos os textos em uma Ãºnica string\n",
    "    texto_completo = ' '.join(dados)\n",
    "\n",
    "    # Criar a nuvem de palavras\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)\n",
    "\n",
    "    # Plotar a nuvem de palavras\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salvar a imagem da nuvem de palavras em um arquivo\n",
    "    imagem_nuvem = 'nuvem_palavras.png'\n",
    "    plt.savefig(imagem_nuvem)\n",
    "\n",
    "    # Retornar o nome do arquivo da imagem\n",
    "    return jsonify({\"imagem_nuvem\": imagem_nuvem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0e9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from flask import jsonify\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "@app.route('/top-palavras', methods=['POST'])\n",
    "def top_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "\n",
    "    # Unir todos os textos em uma Ãºnica string\n",
    "    texto_completo = ' '.join(dados)\n",
    "\n",
    "    # Tokenizar o texto em palavras\n",
    "    tokens = word_tokenize(texto_completo)\n",
    "\n",
    "    # Remover stopwords das palavras tokenizadas\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Contar a ocorrÃªncia das palavras\n",
    "    contagem_palavras = Counter(tokens_filtrados)\n",
    "\n",
    "    # Obter as top 10 palavras mais frequentes\n",
    "    top_palavras = contagem_palavras.most_common(10)\n",
    "\n",
    "    # Retornar as top 10 palavras em formato JSON\n",
    "    return jsonify({\"top_palavras\": top_palavras})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [14/Jun/2023 17:45:14] \"POST /top-palavras HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501b344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
