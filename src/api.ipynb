{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb502b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f110e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeee0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    'üòÄ': 'feliz ',\n",
    "    'üòÇ': 'risos ',\n",
    "    'üòî': 'triste ',\n",
    "    'üëè': 'palmas ',\n",
    "    'ü•∞': 'am√°vel ',\n",
    "    'üíô': 'cora√ß√£o azul ',\n",
    "    'üôèüèº': 'orando ',\n",
    "    '‚ú®': 'brilhando ',\n",
    "    'ü§Æ': 'nojo ',\n",
    "    'üöÄ': 'foguete ',\n",
    "    'üëø': 'diabo ',\n",
    "    'ü§¢': 'nojo ',\n",
    "    'üî•': 'fogo ',\n",
    "    'üò°': 'f√∫ria ',\n",
    "    'üò†': 'raiva ',\n",
    "    'ü§£': 'rindo ',\n",
    "    'üòÉ': 'feliz ',\n",
    "    'üòé': 'curtindo ',\n",
    "    'üòä': 'feliz ',\n",
    "    'ü§©': 'maravilhado ',\n",
    "    'üòã': 'delicioso ',\n",
    "    'üòÜ': 'risada ',\n",
    "    'üòå': 'calmo ',\n",
    "    'ü§î': 'pensativo ',\n",
    "    'üò∑': 'm√°scara ',\n",
    "    'ü§£': 'muitoRiso ',\n",
    "    'ü•∫': 'carinhoso ',\n",
    "    'üëç': 'positivo ',\n",
    "    'ü§Ø': 'menteExplodida ',\n",
    "    'üòÖ': 'al√≠vio ',\n",
    "    'ü•∞': 'carinhaComCora√ß√£o ',\n",
    "    'üòì': 'suor ',\n",
    "    'üòë': 't√©dio',\n",
    "    'ü§´': 'sil√™ncio',\n",
    "    'ü§ù': 'apertoDeM√£os',\n",
    "    'üòä': 'sorriso',\n",
    "    'üòç': 'apaixonado',\n",
    "    'üò≠': 'choro ',\n",
    "    'ü§ó': 'abra√ßo ',\n",
    "    'üéâ': 'festa ',\n",
    "    'üòé': 'descolado ',\n",
    "    'üò±': 'surpresa ',\n",
    "    'üò¥': 'sono ',\n",
    "    'üôå': 'celebra√ß√£o ',\n",
    "    'ü§î': 'pensativo ',\n",
    "    'üòò': 'beijo ',\n",
    "    'ü•≥': 'festeiro ',\n",
    "    'üôÑ': 'revirarOsOlhos ',\n",
    "    'üòå': 'al√≠vio ',\n",
    "    'ü§´': 'segredo ',\n",
    "    'üòá': 'inocente ',\n",
    "    'üòÇ': 'muitoEngra√ßado ',\n",
    "    'ü§î': 'pensando ',\n",
    "    'üò¥': 'sono ',\n",
    "    'ü§™': 'loucura ',\n",
    "    'üò¢': 'decepcionadoAliviado ',\n",
    "    'üò¨': 'nervoso ',\n",
    "    'üòå': 'al√≠vio',\n",
    "    'üòî': 'triste ',\n",
    "    'üòû': 'desapontado ',\n",
    "    'üò¢': 'choro ',\n",
    "    'üò≠': 'chorando ',\n",
    "    'üò°': 'raiva ',\n",
    "    'ü§Ø': 'mente explodida ',\n",
    "    'üò≥': 'surpreso ',\n",
    "    'üò±': 'gritando ',\n",
    "    'üò®': 'assustado ',\n",
    "    'üò¥': 'sono ',\n",
    "    'ü•±': 'bocejando ',\n",
    "    'ü§¢': 'enjoado ',\n",
    "    'ü§Æ': 'vomitando ',\n",
    "    'ü§ß': 'espirro ',\n",
    "    'ü§í': 'doente ',\n",
    "    'ü§ï': 'machucado ',\n",
    "    'ü§ë': 'dinheiro ',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56695fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_word(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        palavras = texto.split()\n",
    "        texto_processado = []\n",
    "        for palavra in palavras:\n",
    "            if palavra in emoji_dict:\n",
    "                texto_processado.append(emoji_dict[palavra])\n",
    "            else:\n",
    "                texto_processado.append(palavra)\n",
    "        texto_processado = ' '.join(texto_processado)\n",
    "        textos_processados.append(texto_processado)\n",
    "    \n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88c240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarTexto(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        texto = texto.lower()\n",
    "        tokens = word_tokenize(texto)\n",
    "        stop_words = [\n",
    "            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',\n",
    "            'n', 'a', '√†', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',\n",
    "            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',\n",
    "            'aquelas', 'aqui', 'aquilo', 'c√°', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',\n",
    "            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',\n",
    "            'n√≥s', 'voc√™s', 'voces', 'enquanto', 'era', 'est√°', 'estamos', 'est√£o', 'estar', 'estar√°', 'estive', 'estivemos',\n",
    "            'estiver', 'estivera', 'estiveram', 'estiv√©ramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',\n",
    "            'estiv√©ssemos', 'estiveste', 'estivestes', 'estou', 'far√°', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',\n",
    "            'for', 'fora', 'foram', 'f√¥ramos', 'forem', 'formos', 'fosse', 'fossem', 'f√¥ssemos', 'foste', 'fostes', 'fui',\n",
    "            'f√¥ssemos', 'h√°', 'houve', 'hoje', 'isso', 'isto', 'j√°', 'l√°', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',\n",
    "            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',\n",
    "            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',\n",
    "            'ningu√©m', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',\n",
    "            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'por√©m', 'porque', 'porqu√™', 'pr√≥pria', 'pr√≥prio',\n",
    "            'pr√≥prias', 'pr√≥prios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'qu√™', 'quem', 'quer', 'quereis',\n",
    "            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quis√©ramos', 'quiserem',\n",
    "            'quisermos', 'quis√©sseis', 'quis√©ssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',\n",
    "            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quiz√©ssemos', 's√£o', 'se', 'seja', 'sejam',\n",
    "            'sejamos', 'sem', 'sendo', 'ser', 'ser√°', 'ser√£o', 'ser√°', 'seriam', 'ser√≠amos', 'serias', 'ser√≠eis', 'sete',\n",
    "            'seu', 'seus', 'sob', 'sobre', 'sois', 's√≥', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'tamb√©m', 'te',\n",
    "            'tem', 't√™m', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'ter√°', 'ter√£o',\n",
    "            'ter√°', 'teriam', 'ter√≠amos', 'terias', 'ter√≠eis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',\n",
    "            'tiveram', 'tiv√©ramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tiv√©ssemos', 'tiveste', 'tivestes',\n",
    "            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns'\n",
    "        ]\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in stop_words and not token.startswith('@') and token.isalpha()\n",
    "        ]\n",
    "        textos_processados.append(tokens)\n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7f9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizar_frases(frases, dictionary):\n",
    "    frases = [' '.join(tokens) for tokens in frases]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.set_params(vocabulary=dictionary)\n",
    "    frases_vetorizadas = vectorizer.fit_transform(frases)\n",
    "    return frases_vetorizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b435bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_naive_bayes.pkl', 'rb') as file:\n",
    "    modelo_carregado = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dicion√°rio usando pickle\n",
    "with open('dictionary.pkl', 'rb') as file:\n",
    "    dictionary2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c57da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e7b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@app.route('/classificar', methods=['POST'])\n",
    "def classificar():\n",
    "    dados = request.json\n",
    "    # Aplique a fun√ß√£o emoji_to_word() aos dados do web scraping\n",
    "    textos_entrada = dados[\"dados\"]\n",
    "    textos_processados = emoji_to_word(textos_entrada)\n",
    "    textos_processados = processarTexto(textos_processados)\n",
    "    frases_vetorizadas = vetorizar_frases(textos_processados, dictionary2)\n",
    "    frases_vetorizadas = frases_vetorizadas.toarray()  # Converter para matriz densa\n",
    "    predicoes = modelo_carregado.predict(frases_vetorizadas)\n",
    "\n",
    "    # Mapear valores num√©ricos para palavras correspondentes\n",
    "    mapeamento_classes = {0: \"negativo\", 1: \"neutro\", 2: \"positivo\"}\n",
    "    predicoes_palavras = [mapeamento_classes[predicao] for predicao in predicoes]\n",
    "\n",
    "    return json.dumps(predicoes_palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3000c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/classificacao', methods=['POST'])\n",
    "def classificar():\n",
    "    dados = request.json[\"dados\"]\n",
    "    \n",
    "    # Aplicar as transforma√ß√µes necess√°rias na entrada\n",
    "    entrada_processada = emoji_to_word(dados)\n",
    "    texto_processado = processarTexto(entrada_processada)\n",
    "    frases_vetorizadas = vetorizar_frases(texto_processado, dictionary2)\n",
    "    frases_vetorizadas = frases_vetorizadas.toarray()\n",
    "    \n",
    "    # Realizar a classifica√ß√£o usando o modelo carregado\n",
    "    predicoes = modelo_carregado.predict(frases_vetorizadas)\n",
    "    \n",
    "    # Contar a ocorr√™ncia de cada classifica√ß√£o\n",
    "    proporcoes = dict(Counter(predicoes))\n",
    "    \n",
    "    # Calcular as propor√ß√µes em porcentagem\n",
    "    total = len(predicoes)\n",
    "    proporcoes_percentual = {classe: (count/total) * 100 for classe, count in proporcoes.items()}\n",
    "    \n",
    "    return jsonify(proporcoes_percentual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db97b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@app.route('/nuvem-palavras', methods=['POST'])\n",
    "def nuvem_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "\n",
    "    # Aplicar a fun√ß√£o emoji_to_word aos dados\n",
    "    dados_processados = emoji_to_word(dados)\n",
    "\n",
    "    # Aplicar a fun√ß√£o processarTexto aos dados\n",
    "    dados_processados = processarTexto(dados_processados)\n",
    "\n",
    "    # Unir todos os textos em uma √∫nica lista de palavras\n",
    "    palavras = [palavra for texto in dados_processados for palavra in texto]\n",
    "\n",
    "    # Remover stopwords das palavras\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "\n",
    "    \n",
    "    # Unir todos os textos em uma √∫nica string\n",
    "    texto_completo = ' '.join(palavras_filtradas)\n",
    "\n",
    "    # Criar a nuvem de palavras\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)\n",
    "\n",
    "    # Plotar a nuvem de palavras\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salvar a imagem da nuvem de palavras em um arquivo\n",
    "    imagem_nuvem = 'nuvem_palavras.png'\n",
    "    plt.savefig(imagem_nuvem)\n",
    "\n",
    "    # Retornar o nome do arquivo da imagem\n",
    "    return jsonify({\"imagem_nuvem\": imagem_nuvem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0e9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from flask import jsonify\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "@app.route('/top-palavras', methods=['POST'])\n",
    "def top_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "\n",
    "    # Aplicar a fun√ß√£o emoji_to_word aos dados\n",
    "    dados_processados = emoji_to_word(dados)\n",
    "\n",
    "    # Aplicar a fun√ß√£o processarTexto aos dados\n",
    "    dados_processados = processarTexto(dados_processados)\n",
    "\n",
    "    # Unir todos os textos em uma √∫nica lista de palavras\n",
    "    palavras = [palavra for texto in dados_processados for palavra in texto]\n",
    "\n",
    "    # Remover stopwords das palavras\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "\n",
    "    # Contar a ocorr√™ncia das palavras\n",
    "    contagem_palavras = Counter(palavras_filtradas)\n",
    "\n",
    "    # Obter as top 10 palavras mais frequentes\n",
    "    top_palavras = contagem_palavras.most_common(10)\n",
    "\n",
    "    # Retornar as top 10 palavras em formato JSON\n",
    "    return jsonify({\"top_palavras\": top_palavras})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f963c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import jsonify\n",
    "\n",
    "@app.route('/maiores-correlacoes', methods=['POST'])\n",
    "def maiores_correlacoes():\n",
    "    dados = request.json[\"dados\"]\n",
    "\n",
    "    # Aplicar a fun√ß√£o emoji_to_word aos dados\n",
    "    dados_processados = emoji_to_word(dados)\n",
    "\n",
    "    # Aplicar a fun√ß√£o processarTexto aos dados\n",
    "    dados_processados = processarTexto(dados_processados)\n",
    "\n",
    "    # Unir todos os textos em uma √∫nica lista de palavras\n",
    "    palavras = [palavra for texto in dados_processados for palavra in texto if palavra != 'n√£o']\n",
    "\n",
    "    # Contar a frequ√™ncia das palavras\n",
    "    contagem_palavras = {}\n",
    "    for palavra in palavras:\n",
    "        if palavra in contagem_palavras:\n",
    "            contagem_palavras[palavra] += 1\n",
    "        else:\n",
    "            contagem_palavras[palavra] = 1\n",
    "\n",
    "    # Encontrar as palavras que mais aparecem na entrada\n",
    "    palavras_mais_frequentes = sorted(contagem_palavras.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "    # Criar um dicion√°rio para armazenar as correla√ß√µes das palavras mais frequentes\n",
    "    maiores_correlacoes = {}\n",
    "\n",
    "    # Encontrar as duas palavras que mais aparecem junto a cada palavra mais frequente\n",
    "    for palavra, _ in palavras_mais_frequentes:\n",
    "        palavras_relacionadas = [palavra]\n",
    "        palavras_relacionadas.extend(\n",
    "            sorted([p for p in palavras if p != palavra and p in contagem_palavras],\n",
    "                   key=lambda p: contagem_palavras[p],\n",
    "                   reverse=True)[:2]\n",
    "        )\n",
    "        maiores_correlacoes[palavra] = palavras_relacionadas\n",
    "\n",
    "    # Retornar as maiores correla√ß√µes em formato JSON\n",
    "    return jsonify({\"maiores_correlacoes\": maiores_correlacoes})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [15/Jun/2023 15:23:15] \"POST /proporcoes HTTP/1.1\" 404 -\n",
      "[2023-06-15 15:23:31,702] ERROR in app: Exception on /classificacao [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Temp\\ipykernel_27208\\775858374.py\", line 27, in classificar\n",
      "    return jsonify(proporcoes_percentual)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\json\\__init__.py\", line 170, in jsonify\n",
      "    return current_app.json.response(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\json\\provider.py\", line 215, in response\n",
      "    f\"{self.dumps(obj, **dump_args)}\\n\", mimetype=self.mimetype\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\json\\provider.py\", line 180, in dumps\n",
      "    return json.dumps(obj, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\encoder.py\", line 200, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Inteli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\encoder.py\", line 258, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "TypeError: keys must be str, int, float, bool or None, not numpy.int32\n",
      "127.0.0.1 - - [15/Jun/2023 15:23:31] \"POST /classificacao HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501b344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
