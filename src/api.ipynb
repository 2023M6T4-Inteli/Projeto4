{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc8c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb502b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f110e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: emoji in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inteli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeee0cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "    '😀': 'feliz ',\n",
    "    '😂': 'risos ',\n",
    "    '😔': 'triste ',\n",
    "    '👏': 'palmas ',\n",
    "    '🥰': 'amável ',\n",
    "    '💙': 'coração azul ',\n",
    "    '🙏🏼': 'orando ',\n",
    "    '✨': 'brilhando ',\n",
    "    '🤮': 'nojo ',\n",
    "    '🚀': 'foguete ',\n",
    "    '👿': 'diabo ',\n",
    "    '🤢': 'nojo ',\n",
    "    '🔥': 'fogo ',\n",
    "    '😡': 'fúria ',\n",
    "    '😠': 'raiva ',\n",
    "    '🤣': 'rindo ',\n",
    "    '😃': 'feliz ',\n",
    "    '😎': 'curtindo ',\n",
    "    '😊': 'feliz ',\n",
    "    '🤩': 'maravilhado ',\n",
    "    '😋': 'delicioso ',\n",
    "    '😆': 'risada ',\n",
    "    '😌': 'calmo ',\n",
    "    '🤔': 'pensativo ',\n",
    "    '😷': 'máscara ',\n",
    "    '🤣': 'muitoRiso ',\n",
    "    '🥺': 'carinhoso ',\n",
    "    '👍': 'positivo ',\n",
    "    '🤯': 'menteExplodida ',\n",
    "    '😅': 'alívio ',\n",
    "    '🥰': 'carinhaComCoração ',\n",
    "    '😓': 'suor ',\n",
    "    '😑': 'tédio',\n",
    "    '🤫': 'silêncio',\n",
    "    '🤝': 'apertoDeMãos',\n",
    "    '😊': 'sorriso',\n",
    "    '😍': 'apaixonado',\n",
    "    '😭': 'choro ',\n",
    "    '🤗': 'abraço ',\n",
    "    '🎉': 'festa ',\n",
    "    '😎': 'descolado ',\n",
    "    '😱': 'surpresa ',\n",
    "    '😴': 'sono ',\n",
    "    '🙌': 'celebração ',\n",
    "    '🤔': 'pensativo ',\n",
    "    '😘': 'beijo ',\n",
    "    '🥳': 'festeiro ',\n",
    "    '🙄': 'revirarOsOlhos ',\n",
    "    '😌': 'alívio ',\n",
    "    '🤫': 'segredo ',\n",
    "    '😇': 'inocente ',\n",
    "    '😂': 'muitoEngraçado ',\n",
    "    '🤔': 'pensando ',\n",
    "    '😴': 'sono ',\n",
    "    '🤪': 'loucura ',\n",
    "    '😢': 'decepcionadoAliviado ',\n",
    "    '😬': 'nervoso ',\n",
    "    '😌': 'alívio',\n",
    "    '😔': 'triste ',\n",
    "    '😞': 'desapontado ',\n",
    "    '😢': 'choro ',\n",
    "    '😭': 'chorando ',\n",
    "    '😡': 'raiva ',\n",
    "    '🤯': 'mente explodida ',\n",
    "    '😳': 'surpreso ',\n",
    "    '😱': 'gritando ',\n",
    "    '😨': 'assustado ',\n",
    "    '😴': 'sono ',\n",
    "    '🥱': 'bocejando ',\n",
    "    '🤢': 'enjoado ',\n",
    "    '🤮': 'vomitando ',\n",
    "    '🤧': 'espirro ',\n",
    "    '🤒': 'doente ',\n",
    "    '🤕': 'machucado ',\n",
    "    '🤑': 'dinheiro ',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56695fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_to_word(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        palavras = texto.split()\n",
    "        texto_processado = []\n",
    "        for palavra in palavras:\n",
    "            if palavra in emoji_dict:\n",
    "                texto_processado.append(emoji_dict[palavra])\n",
    "            else:\n",
    "                texto_processado.append(palavra)\n",
    "        texto_processado = ' '.join(texto_processado)\n",
    "        textos_processados.append(texto_processado)\n",
    "    \n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88c240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processarTexto(textos):\n",
    "    textos_processados = []\n",
    "    for texto in textos:\n",
    "        texto = texto.lower()\n",
    "        tokens = word_tokenize(texto)\n",
    "        stop_words = [\n",
    "            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',\n",
    "            'n', 'a', 'à', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',\n",
    "            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',\n",
    "            'aquelas', 'aqui', 'aquilo', 'cá', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',\n",
    "            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',\n",
    "            'nós', 'vocês', 'voces', 'enquanto', 'era', 'está', 'estamos', 'estão', 'estar', 'estará', 'estive', 'estivemos',\n",
    "            'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',\n",
    "            'estivéssemos', 'estiveste', 'estivestes', 'estou', 'fará', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',\n",
    "            'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui',\n",
    "            'fôssemos', 'há', 'houve', 'hoje', 'isso', 'isto', 'já', 'lá', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',\n",
    "            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',\n",
    "            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',\n",
    "            'ninguém', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',\n",
    "            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'própria', 'próprio',\n",
    "            'próprias', 'próprios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'quê', 'quem', 'quer', 'quereis',\n",
    "            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quiséramos', 'quiserem',\n",
    "            'quisermos', 'quisésseis', 'quiséssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',\n",
    "            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quizéssemos', 'são', 'se', 'seja', 'sejam',\n",
    "            'sejamos', 'sem', 'sendo', 'ser', 'será', 'serão', 'será', 'seriam', 'seríamos', 'serias', 'seríeis', 'sete',\n",
    "            'seu', 'seus', 'sob', 'sobre', 'sois', 'só', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'te',\n",
    "            'tem', 'têm', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão',\n",
    "            'terá', 'teriam', 'teríamos', 'terias', 'teríeis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',\n",
    "            'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes',\n",
    "            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns'\n",
    "        ]\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in stop_words and not token.startswith('@') and token.isalpha()\n",
    "        ]\n",
    "        textos_processados.append(tokens)\n",
    "    return textos_processados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a7f9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizar_frases(frases, dictionary):\n",
    "    frases = [' '.join(tokens) for tokens in frases]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.set_params(vocabulary=dictionary)\n",
    "    frases_vetorizadas = vectorizer.fit_transform(frases)\n",
    "    return frases_vetorizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b435bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_naive_bayes.pkl', 'rb') as file:\n",
    "    modelo_carregado = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0c793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dicionário usando pickle\n",
    "with open('dictionary.pkl', 'rb') as file:\n",
    "    dictionary2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c57da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e7b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@app.route('/classificar', methods=['POST'])\n",
    "def classificar():\n",
    "    dados = request.json\n",
    "    # Aplique a função emoji_to_word() aos dados do web scraping\n",
    "    textos_entrada = dados[\"dados\"]\n",
    "    textos_processados = emoji_to_word(textos_entrada)\n",
    "    textos_processados = processarTexto(textos_processados)\n",
    "    frases_vetorizadas = vetorizar_frases(textos_processados, dictionary2)\n",
    "    frases_vetorizadas = frases_vetorizadas.toarray()  # Converter para matriz densa\n",
    "    predicoes = modelo_carregado.predict(frases_vetorizadas)\n",
    "\n",
    "    # Mapear valores numéricos para palavras correspondentes\n",
    "    mapeamento_classes = {0: \"negativo\", 1: \"neutro\", 2: \"positivo\"}\n",
    "    predicoes_palavras = [mapeamento_classes[predicao] for predicao in predicoes]\n",
    "\n",
    "    return json.dumps(predicoes_palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3000c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "@app.route('/proporcoes', methods=['POST'])\n",
    "def proporcoes():\n",
    "    predicoes_palavras = request.json\n",
    "    predicoes_palavras = predicoes_palavras[\"dados\"]\n",
    "\n",
    "    # Mapear os sentimentos para os valores numéricos correspondentes\n",
    "    mapeamento_sentimentos = {\"negativo\": 0, \"neutro\": 1, \"positivo\": 2}\n",
    "    predicoes_numeros = [mapeamento_sentimentos[sentimento] for sentimento in predicoes_palavras]\n",
    "\n",
    "    # Contar a ocorrência de cada sentimento\n",
    "    proporcoes = Counter(predicoes_numeros)\n",
    "\n",
    "    # Calcular as proporções\n",
    "    total = len(predicoes_numeros)\n",
    "    proporcoes = {sentimento: count/total for sentimento, count in proporcoes.items()}\n",
    "\n",
    "    return json.dumps(proporcoes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db97b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@app.route('/nuvem-palavras', methods=['POST'])\n",
    "def nuvem_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "    \n",
    "    # Unir todos os textos em uma única string\n",
    "    texto_completo = ' '.join(dados)\n",
    "\n",
    "    # Criar a nuvem de palavras\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)\n",
    "\n",
    "    # Plotar a nuvem de palavras\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Salvar a imagem da nuvem de palavras em um arquivo\n",
    "    imagem_nuvem = 'nuvem_palavras.png'\n",
    "    plt.savefig(imagem_nuvem)\n",
    "\n",
    "    # Retornar o nome do arquivo da imagem\n",
    "    return jsonify({\"imagem_nuvem\": imagem_nuvem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0e9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from flask import jsonify\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "@app.route('/top-palavras', methods=['POST'])\n",
    "def top_palavras():\n",
    "    dados = request.json[\"dados\"]\n",
    "\n",
    "    # Unir todos os textos em uma única string\n",
    "    texto_completo = ' '.join(dados)\n",
    "\n",
    "    # Tokenizar o texto em palavras\n",
    "    tokens = word_tokenize(texto_completo)\n",
    "\n",
    "    # Remover stopwords das palavras tokenizadas\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens_filtrados = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Contar a ocorrência das palavras\n",
    "    contagem_palavras = Counter(tokens_filtrados)\n",
    "\n",
    "    # Obter as top 10 palavras mais frequentes\n",
    "    top_palavras = contagem_palavras.most_common(10)\n",
    "\n",
    "    # Retornar as top 10 palavras em formato JSON\n",
    "    return jsonify({\"top_palavras\": top_palavras})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4b588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [14/Jun/2023 17:45:14] \"POST /top-palavras HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501b344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
